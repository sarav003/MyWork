{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Tfidf vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['this is the first document',\n",
    "      'this document is the second document',\n",
    "      'and this is the third one',\n",
    "      'is this the first document', ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit function for Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF(corpus, unique_words):\n",
    "    idf_dict={}\n",
    "    N=len(corpus)\n",
    "    for i in unique_words:\n",
    "        count=0\n",
    "        for sen in corpus:\n",
    "            if i in sen.split():\n",
    "                count=count+1\n",
    "            idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(whole_data):\n",
    "    unique_words = set()\n",
    "    if isinstance(whole_data, (list,)):\n",
    "        for x in whole_data:\n",
    "            for y in x.split():\n",
    "                if len(y)<2:\n",
    "                    continue\n",
    "                unique_words.add(y)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "    Idf_values_of_unique_words=IDF(whole_data,unique_words)\n",
    "    return vocab, Idf_values_of_unique_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n"
     ]
    }
   ],
   "source": [
    "Vocab, idf_of_vocab=fit(corpus)\n",
    "print(Vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Both the unique words are same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset,vocab,idf_values):\n",
    "    sparse= csr_matrix( (len(dataset), len(vocab)), dtype=np.float64)\n",
    "    for row  in range(0,len(dataset)):\n",
    "        number_of_words_in_sentence=Counter(dataset[row].split())\n",
    "        for word in dataset[row].split():\n",
    "            if word in  list(vocab.keys()):\n",
    "                tf_idf_value=(number_of_words_in_sentence[word]/len(dataset[row].split()))*(idf_values[word])\n",
    "                sparse[row,vocab[word]]=tf_idf_value\n",
    "    output =normalize(sparse, norm='l2', axis=1, copy=True, return_norm=False)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.580285823684436\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n",
      "  (1, 1)\t0.6876235979836937\n",
      "  (1, 3)\t0.2810886740337529\n",
      "  (1, 5)\t0.5386476208856762\n",
      "  (1, 6)\t0.2810886740337529\n",
      "  (1, 8)\t0.2810886740337529\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (3, 1)\t0.4697913855799205\n",
      "  (3, 2)\t0.580285823684436\n",
      "  (3, 3)\t0.3840852409148149\n",
      "  (3, 6)\t0.3840852409148149\n",
      "  (3, 8)\t0.3840852409148149\n",
      "the shape of the sparse matrix (4, 9)\n"
     ]
    }
   ],
   "source": [
    "fin_out=transform(corpus,Vocab,idf_of_vocab)\n",
    "print(fin_out)\n",
    "print(\"the shape of the sparse matrix\",fin_out.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "print(fin_out[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn's Countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output=vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n",
      "  (1, 8)\t0.281088674033753\n",
      "  (1, 6)\t0.281088674033753\n",
      "  (1, 5)\t0.5386476208856763\n",
      "  (1, 3)\t0.281088674033753\n",
      "  (1, 1)\t0.6876235979836938\n",
      "  (2, 8)\t0.267103787642168\n",
      "  (2, 7)\t0.511848512707169\n",
      "  (2, 6)\t0.267103787642168\n",
      "  (2, 4)\t0.511848512707169\n",
      "  (2, 3)\t0.267103787642168\n",
      "  (2, 0)\t0.511848512707169\n",
      "  (3, 8)\t0.38408524091481483\n",
      "  (3, 6)\t0.38408524091481483\n",
      "  (3, 3)\t0.38408524091481483\n",
      "  (3, 2)\t0.5802858236844359\n",
      "  (3, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "print(skl_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "print(skl_output[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF values for all the words are the same for both programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider only the top 50 feature names based on their IDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus1 = pickle.load(f)\n",
    "print(\"Number of documents in corpus = \",len(corpus1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDF(corpus, unique_words):\n",
    "    a=0\n",
    "    idf_dict={}\n",
    "    idfsorted={}\n",
    "    unique_words50={}\n",
    "    idf50={}\n",
    "    N=len(corpus)\n",
    "    for i in unique_words:\n",
    "        count=0\n",
    "        for sen in corpus:\n",
    "            if i in sen.split():\n",
    "                count=count+1\n",
    "            idf_dict[i]=(math.log((1+N)/(count+1)))+1\n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(whole_data):\n",
    "    unique_words = set()\n",
    "    if isinstance(whole_data, (list,)):\n",
    "        for x in whole_data:\n",
    "            for y in x.split():\n",
    "                if len(y)<2:\n",
    "                    continue\n",
    "                unique_words.add(y)\n",
    "        unique_words = sorted(list(unique_words))\n",
    "       \n",
    "    Idf_values_of_all_unique_words=IDF(whole_data,unique_words)\n",
    "    \n",
    "    top50_idf_value_indices = np.argsort(list(Idf_values_of_all_unique_words.values()))[::-1][:50]\n",
    "    \n",
    "    top50_idf_values = np.take(list(Idf_values_of_all_unique_words.values()), top50_idf_value_indices)\n",
    "    top50_idf_words = np.take(list(Idf_values_of_all_unique_words.keys()), top50_idf_value_indices)\n",
    "\n",
    "    want_idf_vocabulary = dict(zip(top50_idf_words, top50_idf_values))\n",
    "    want_vocabulary = {j: i for i, j in enumerate(list(want_idf_vocabulary.keys()))}\n",
    "    return want_vocabulary, want_idf_vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'zombiez': 0, 'havilland': 1, 'hearts': 2, 'heads': 3, 'hbo': 4, 'hayworth': 5, 'hayao': 6, 'hay': 7, 'hatred': 8, 'heche': 9, 'harris': 10, 'happy': 11, 'happiness': 12, 'hanks': 13, 'hankies': 14, 'hang': 15, 'heartwarming': 16, 'heels': 17, 'gone': 18, 'hero': 19, 'higher': 20, 'hide': 21, 'hes': 22, 'heroism': 23, 'heroine': 24, 'heroes': 25, 'hernandez': 26, 'heist': 27, 'hendrikson': 28, 'helping': 29, 'help': 30, 'helms': 31, 'hellish': 32, 'helen': 33, 'handles': 34, 'handle': 35, 'ham': 36, 'grade': 37, 'grates': 38, 'grasp': 39, 'graphics': 40, 'granted': 41, 'grainy': 42, 'gradually': 43, 'government': 44, 'halfway': 45, 'gotten': 46, 'gotta': 47, 'goth': 48, 'gosh': 49}\n"
     ]
    }
   ],
   "source": [
    "want_vocabulary, want_idf_vocabulary = fit(corpus1)\n",
    "print(want_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'zombiez': 6.922918004572872, 'havilland': 6.922918004572872, 'hearts': 6.922918004572872, 'heads': 6.922918004572872, 'hbo': 6.922918004572872, 'hayworth': 6.922918004572872, 'hayao': 6.922918004572872, 'hay': 6.922918004572872, 'hatred': 6.922918004572872, 'heche': 6.922918004572872, 'harris': 6.922918004572872, 'happy': 6.922918004572872, 'happiness': 6.922918004572872, 'hanks': 6.922918004572872, 'hankies': 6.922918004572872, 'hang': 6.922918004572872, 'heartwarming': 6.922918004572872, 'heels': 6.922918004572872, 'gone': 6.922918004572872, 'hero': 6.922918004572872, 'higher': 6.922918004572872, 'hide': 6.922918004572872, 'hes': 6.922918004572872, 'heroism': 6.922918004572872, 'heroine': 6.922918004572872, 'heroes': 6.922918004572872, 'hernandez': 6.922918004572872, 'heist': 6.922918004572872, 'hendrikson': 6.922918004572872, 'helping': 6.922918004572872, 'help': 6.922918004572872, 'helms': 6.922918004572872, 'hellish': 6.922918004572872, 'helen': 6.922918004572872, 'handles': 6.922918004572872, 'handle': 6.922918004572872, 'ham': 6.922918004572872, 'grade': 6.922918004572872, 'grates': 6.922918004572872, 'grasp': 6.922918004572872, 'graphics': 6.922918004572872, 'granted': 6.922918004572872, 'grainy': 6.922918004572872, 'gradually': 6.922918004572872, 'government': 6.922918004572872, 'halfway': 6.922918004572872, 'gotten': 6.922918004572872, 'gotta': 6.922918004572872, 'goth': 6.922918004572872, 'gosh': 6.922918004572872}\n"
     ]
    }
   ],
   "source": [
    "print(want_idf_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(dataset,vocab,idf_values):\n",
    "    sparse= csr_matrix( (len(dataset), len(vocab)), dtype=np.float64)\n",
    "    for row  in range(0,len(dataset)):\n",
    "        number_of_words_in_sentence=Counter(dataset[row].split())\n",
    "        for word in dataset[row].split():\n",
    "            if word in  list(vocab.keys()):\n",
    "                tf_idf_value=(number_of_words_in_sentence[word]/len(dataset[row].split()))*(idf_values[word])\n",
    "                sparse[row,vocab[word]]=tf_idf_value\n",
    "    output =normalize(sparse, norm='l2', axis=1, copy=True, return_norm=False)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(746, 50)\n",
      "  (19, 9)\t0.25819888974716115\n",
      "  (19, 13)\t0.25819888974716115\n",
      "  (19, 17)\t0.25819888974716115\n",
      "  (19, 30)\t0.25819888974716115\n",
      "  (19, 31)\t0.25819888974716115\n",
      "  (19, 38)\t0.25819888974716115\n",
      "  (19, 40)\t0.7745966692414833\n",
      "  (94, 49)\t1.0\n",
      "  (101, 18)\t1.0\n",
      "  (104, 35)\t1.0\n",
      "  (109, 0)\t0.7071067811865476\n",
      "  (109, 32)\t0.7071067811865476\n",
      "  (132, 25)\t1.0\n",
      "  (135, 12)\t0.5773502691896258\n",
      "  (135, 43)\t0.5773502691896258\n",
      "  (135, 46)\t0.5773502691896258\n",
      "  (180, 20)\t1.0\n",
      "  (191, 7)\t0.7071067811865475\n",
      "  (191, 24)\t0.7071067811865475\n",
      "  (197, 5)\t1.0\n",
      "  (222, 22)\t1.0\n",
      "  (225, 8)\t1.0\n",
      "  (232, 21)\t1.0\n",
      "  (234, 45)\t1.0\n",
      "  (236, 28)\t1.0\n",
      "  (253, 15)\t1.0\n",
      "  (270, 27)\t1.0\n",
      "  (277, 48)\t1.0\n",
      "  (323, 37)\t1.0\n",
      "  (343, 41)\t1.0\n",
      "  (371, 47)\t1.0\n",
      "  (421, 23)\t1.0\n",
      "  (430, 1)\t1.0\n",
      "  (437, 19)\t1.0\n",
      "  (459, 33)\t1.0\n",
      "  (462, 29)\t1.0\n",
      "  (475, 14)\t1.0\n",
      "  (532, 34)\t1.0\n",
      "  (533, 11)\t1.0\n",
      "  (539, 36)\t1.0\n",
      "  (572, 10)\t1.0\n",
      "  (610, 42)\t1.0\n",
      "  (625, 3)\t1.0\n",
      "  (628, 26)\t1.0\n",
      "  (633, 39)\t1.0\n",
      "  (644, 16)\t1.0\n",
      "  (660, 4)\t1.0\n",
      "  (681, 2)\t1.0\n",
      "  (703, 44)\t1.0\n",
      "  (714, 6)\t1.0\n"
     ]
    }
   ],
   "source": [
    "final_output = transform(corpus1, want_vocabulary, want_idf_vocabulary)\n",
    "print(final_output.shape)\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t0.25819888974716115\n",
      "  (0, 13)\t0.25819888974716115\n",
      "  (0, 17)\t0.25819888974716115\n",
      "  (0, 30)\t0.25819888974716115\n",
      "  (0, 31)\t0.25819888974716115\n",
      "  (0, 38)\t0.25819888974716115\n",
      "  (0, 40)\t0.7745966692414833\n"
     ]
    }
   ],
   "source": [
    "print(final_output[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.25819889 0.         0.\n",
      "  0.         0.25819889 0.         0.         0.         0.25819889\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.25819889 0.25819889 0.         0.         0.         0.\n",
      "  0.         0.         0.25819889 0.         0.77459667 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(final_output[19].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn's countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 25)\t1.0\n",
      "  (1, 30)\t0.572445699981522\n",
      "  (1, 8)\t0.8199426324888012\n",
      "  (2, 32)\t0.5699230219852334\n",
      "  (2, 25)\t0.35548195762874873\n",
      "  (2, 10)\t0.5273651853196742\n",
      "  (2, 0)\t0.5202944244602254\n",
      "  (3, 20)\t1.0\n",
      "  (4, 25)\t0.5110768901174535\n",
      "  (4, 4)\t0.859534997767905\n",
      "  (5, 25)\t1.0\n",
      "  (7, 25)\t0.33776010837475723\n",
      "  (7, 17)\t0.9412322291499969\n",
      "  (11, 25)\t0.32556342048857684\n",
      "  (11, 23)\t0.5366405664621315\n",
      "  (11, 21)\t0.5533799348242362\n",
      "  (11, 4)\t0.5475363087512142\n",
      "  (12, 4)\t1.0\n",
      "  (14, 25)\t1.0\n",
      "  (15, 37)\t0.7118629038754104\n",
      "  (15, 0)\t0.7023184506234106\n",
      "  (16, 23)\t0.6478335937139895\n",
      "  (16, 14)\t0.4007783298922675\n",
      "  (16, 11)\t0.6478335937139895\n",
      "  (17, 25)\t0.48899424176935075\n",
      "  :\t:\n",
      "  (727, 44)\t1.0\n",
      "  (728, 25)\t0.5830722125629216\n",
      "  (728, 17)\t0.8124203314399383\n",
      "  (729, 38)\t0.7482164865280394\n",
      "  (729, 30)\t0.4838684403668721\n",
      "  (729, 25)\t0.45392006091884396\n",
      "  (733, 46)\t0.8346729885507431\n",
      "  (733, 25)\t0.5507458598879985\n",
      "  (734, 37)\t0.6615139054580876\n",
      "  (734, 33)\t0.7499328989220224\n",
      "  (735, 35)\t0.7391763947283081\n",
      "  (735, 32)\t0.6735118836935696\n",
      "  (736, 43)\t1.0\n",
      "  (738, 18)\t1.0\n",
      "  (739, 47)\t0.6094124262000808\n",
      "  (739, 40)\t0.5963565650047202\n",
      "  (739, 19)\t0.5224704222907762\n",
      "  (740, 28)\t0.8668308595259199\n",
      "  (740, 25)\t0.4986023074290322\n",
      "  (741, 44)\t1.0\n",
      "  (742, 48)\t0.6520428640510826\n",
      "  (742, 36)\t0.6520428640510826\n",
      "  (742, 14)\t0.3868852631984358\n",
      "  (744, 3)\t1.0\n",
      "  (745, 31)\t1.0\n",
      "(746, 50)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer(norm = 'l2',max_features=50)\n",
    "vect=vectorizer.fit(corpus1)\n",
    "skl_output=vectorizer.transform(corpus1)\n",
    "print(skl_output)\n",
    "print(skl_output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
